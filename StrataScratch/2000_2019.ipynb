{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2000\n",
    "\n",
    "```Write a query that returns binary description of rate type per loan_id. The results should have one row per loan_id and two columns: for fixed and variable type.```"
   ],
   "id": "ad5144064b1dc199"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT loan_id,\n",
    "       CASE WHEN rate_type = 'fixed' THEN 1 ELSE 0 END    AS fixed,\n",
    "       CASE WHEN rate_type = 'variable' THEN 1 ELSE 0 END AS variable\n",
    "FROM submissions;"
   ],
   "id": "113628abdf437a54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = submissions\n",
    "pd.get_dummies(df[['loan_id', 'rate_type']], prefix='', prefix_sep='')"
   ],
   "id": "d2baaae00c85f405"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2001\n",
    "\n",
    "```Write a query that returns the rate_type, loan_id, loan balance , and a column that shows with what percentage the loan's balance contributes to the total balance among the loans of the same rate type```"
   ],
   "id": "9554b4bf8380823c"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql4"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT rate_type,\n",
    "       loan_id,\n",
    "       SUM(balance) OVER (PARTITION BY loan_id)                     AS balance,\n",
    "       balance * 100.0 / SUM(balance) OVER (PARTITION BY rate_type) AS balance_share\n",
    "FROM submissions;"
   ],
   "id": "eeed0cf7ce11cc2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = submissions\n",
    "df.groupby('loan_id', as_index=False).agg(balance=('balance', 'sum'))\n",
    "df = pd.merge(\n",
    "    pd.merge(df, df.groupby('rate_type', as_index=False).agg(balance_by_type=('balance', 'sum')), on='rate_type').drop(\n",
    "        columns='balance'), df.groupby('loan_id', as_index=False).agg(balance=('balance', 'sum')), on='loan_id')\n",
    "df['balance_share'] = df['balance'] * 100 / df['balance_by_type']\n",
    "df[['loan_id', 'rate_type', 'balance', 'balance_share']]"
   ],
   "id": "ebe97bd43037508f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2002\n",
    "\n",
    "```Write a query that returns the user ID of all users that have created at least one ‘Refinance’ submission and at least one ‘InSchool’ submission.```"
   ],
   "id": "38880d09d9843505"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql1"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT DISTINCT user_id\n",
    "FROM loans\n",
    "WHERE user_id IN (SELECT user_id FROM loans WHERE type IN ('Refinance'))\n",
    "  AND user_id IN (SELECT user_id FROM loans WHERE type IN ('InSchool'))"
   ],
   "id": "f08387a59296b906"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_ref = loans.query('type == \"Refinance\"')['user_id'].drop_duplicates()\n",
    "df_sch = loans.query('type == \"InSchool\"')['user_id'].drop_duplicates()\n",
    "df = pd.merge(df_ref, df_sch, on='user_id')"
   ],
   "id": "2d13e96f69982558"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2003\n",
    "\n",
    "```Write a query that joins this submissions table to the loans table and returns the total loan balance on each user’s most recent ‘Refinance’ submission. Return all users and the balance for each of them.```"
   ],
   "id": "689c5a933c4d8ee6"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql6"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "WITH cte AS (SELECT user_id,\n",
    "                    balance,\n",
    "                    DENSE_RANK()\n",
    "                    OVER (PARTITION BY user_id ORDER BY created_at DESC) AS rnk\n",
    "             FROM loans l\n",
    "                      JOIN submissions s ON l.id = s.loan_id\n",
    "             WHERE type = 'Refinance')\n",
    "SELECT user_id, balance\n",
    "FROM cte\n",
    "WHERE rnk = 1"
   ],
   "id": "ed6eb798220ba51c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.merge(loans.query('type == \"Refinance\"'), submissions, how='inner', left_on='id', right_on='loan_id')\n",
    "df['rnk'] = df.groupby('user_id')['created_at'].rank(method='first', ascending=False)\n",
    "df.query('rnk == 1')[['user_id', 'balance']]"
   ],
   "id": "91e28774cff994fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2004\n",
    "\n",
    "```Return the total number of comments received for each user in the 30 or less days before 2020-02-10. Don't output users who haven't received any comment in the defined time period.```"
   ],
   "id": "2eac5d2e2701fd8b"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql7"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT user_id,\n",
    "       SUM(number_of_comments) AS number_of_comments\n",
    "FROM fb_comments_count\n",
    "WHERE ('2020-02-10' - created_at) BETWEEN 0 AND 30\n",
    "GROUP BY user_id"
   ],
   "id": "76c8693d5add1ba9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = fb_comments_count\n",
    "df[(pd.to_datetime('2020-02-10') - df['created_at']).dt.days.between(0, 30)].groupby('user_id', as_index=False).agg(\n",
    "    n_comments=('number_of_comments', 'sum'))"
   ],
   "id": "a7637d7c621563d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2005\n",
    "\n",
    "```Output share of US users that are active. Active users are the ones with an \"open\" status in the table.```"
   ],
   "id": "9287f30ff9bfb98b"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql13"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT COUNT(user_id) FILTER (WHERE status = 'open') * 1.0 / COUNT(user_id) *\n",
    "       1.0 AS active_users_share\n",
    "FROM fb_active_users\n",
    "WHERE country = 'USA'"
   ],
   "id": "1ce8ed6acc613715"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = fb_active_users\n",
    "result = df.query('country == \"USA\" & status == \"open\"')['user_id'].nunique() / df.query('country == \"USA\"')[\n",
    "    'user_id'].nunique()\n",
    "result_df = pd.DataFrame({'index': ['user_count'], 'open': [result]})"
   ],
   "id": "1679aab70149df14"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2006\n",
    "\n",
    "```Return a distribution of users activity per day of the month. By distribution we mean the number of posts per day of the month.```"
   ],
   "id": "4cf4033038ed3ea6"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql8"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT EXTRACT(DAY FROM post_date), COUNT(post_text)\n",
    "FROM facebook_posts\n",
    "GROUP BY EXTRACT(DAY FROM post_date)"
   ],
   "id": "ea6470f213541d01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = facebook_posts\n",
    "df.groupby(df['post_date'].dt.day, as_index=False).agg(count=('post_text', 'count')).to_frame('user_activity')"
   ],
   "id": "f1543dcfe2818283"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2007\n",
    "\n",
    "```Which countries have risen in the rankings based on the number of comments between Dec 2019 vs Jan 2020? Hint: Avoid gaps between ranks when ranking countries!!!```"
   ],
   "id": "7250a99ea32d035d"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql15"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "WITH dec_comments AS (SELECT country,\n",
    "                             SUM(number_of_comments) AS total_comments,\n",
    "                             DENSE_RANK() OVER (ORDER BY SUM(number_of_comments) DESC)\n",
    "                                                     AS rnk\n",
    "                      FROM fb_active_users AS au\n",
    "                               JOIN fb_comments_count AS cc ON cc.user_id = au.user_id\n",
    "                      WHERE EXTRACT(MONTH FROM created_at) = 12\n",
    "                        AND EXTRACT(YEAR FROM created_at) = 2019\n",
    "                      GROUP BY country),\n",
    "     jan_comments AS (SELECT country,\n",
    "                             SUM(number_of_comments) AS total_comments,\n",
    "                             DENSE_RANK() OVER (ORDER BY SUM(number_of_comments) DESC)\n",
    "                                                     AS rnk\n",
    "                      FROM fb_active_users AS au\n",
    "                               JOIN fb_comments_count AS cc ON cc.user_id = au.user_id\n",
    "                      WHERE EXTRACT(MONTH FROM created_at) = 1\n",
    "                        AND EXTRACT(YEAR FROM created_at) = 2020\n",
    "                      GROUP BY country)\n",
    "SELECT j.country\n",
    "FROM jan_comments AS j\n",
    "         FULL JOIN dec_comments AS d ON d.country = j.country\n",
    "WHERE j.rnk < d.rnk\n",
    "  OR d.rnk IS NULL;"
   ],
   "id": "de77e8550c2d497e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_dec_comments = fb_comments_count.query('created_at.dt.year == 2019 & created_at.dt.month == 12').merge(\n",
    "    fb_active_users, how='left', on='user_id').groupby('country', as_index=False).agg(\n",
    "    total_comments=('number_of_comments', 'sum'))\n",
    "df_dec_comments['rnk'] = df_dec_comments['total_comments'].rank(method='dense', ascending=False)\n",
    "\n",
    "df_jan_comments = fb_comments_count.query('created_at.dt.year == 2020 & created_at.dt.month == 1').merge(\n",
    "    fb_active_users, how='left', on='user_id').groupby('country', as_index=False).agg(\n",
    "    total_comments=('number_of_comments', 'sum'))\n",
    "df_jan_comments['rnk'] = df_jan_comments['total_comments'].rank(method='dense', ascending=False)\n",
    "\n",
    "df = pd.merge(df_dec_comments, df_jan_comments, how='outer', on='country', suffixes=('_dec', '_jan')).query(\n",
    "    'rnk_dec > rnk_jan | rnk_dec.isnull()')[['country']]"
   ],
   "id": "ae4a4fde3530e934"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2009\n",
    "\n",
    "```Find users who are both a viewer and streamer.```"
   ],
   "id": "fa81b0091543fae3"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql5"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT DISTINCT user_id\n",
    "FROM twitch_sessions\n",
    "WHERE user_id IN (SELECT user_id FROM twitch_sessions WHERE session_type = 'viewer')\n",
    "  AND user_id IN (SELECT user_id FROM twitch_sessions WHERE session_type = 'streamer')"
   ],
   "id": "6254fff86ed10234"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.merge(twitch_sessions, twitch_sessions, how='inner', on='user_id', suffixes=('_user1', '_user2')).query(\n",
    "    'session_type_user1 == \"streamer\" & session_type_user2 == \"viewer\"')[\n",
    "    'user_id'].drop_duplicates().sort_values().reset_index(drop=True)"
   ],
   "id": "d83d2985b1142b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2010\n",
    "\n",
    "```List the top 10 users who accumulated the most sessions where they had more streaming sessions than viewing. Return the user_id, number of streaming sessions, and number of viewing sessions.```"
   ],
   "id": "40e0c615ffeae9ae"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql2"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "WITH filtered AS (SELECT user_id,\n",
    "                    COUNT(CASE WHEN session_type = 'streamer' THEN 1 ELSE NULL END) AS streaming_sessions,\n",
    "                    COUNT(CASE WHEN session_type = 'viewer' THEN 1 ELSE NULL END)   AS viewing_sessions\n",
    "             FROM twitch_sessions\n",
    "             GROUP BY user_id\n",
    "             HAVING COUNT(CASE WHEN session_type = 'streamer' THEN 1 ELSE NULL END) >\n",
    "                    COUNT(CASE WHEN session_type = 'viewer' THEN 1 ELSE NULL END)),\n",
    "     ranked AS (SELECT user_id,\n",
    "                       streaming_sessions,\n",
    "                       viewing_sessions,\n",
    "                      DENSE_RANK()\n",
    "                      OVER (ORDER BY (streaming_sessions + viewing_sessions) DESC) AS rnk\n",
    "               FROM filtered)\n",
    "SELECT user_id, streaming_sessions, viewing_sessions\n",
    "FROM ranked\n",
    "WHERE rnk <= 10"
   ],
   "id": "238451798ea75152"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TODO",
   "id": "c88e010b5cd0af16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2011\n",
    "\n",
    "```Calculate the average session duration for each session type?```"
   ],
   "id": "8a55a584c101ff43"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql9"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT session_type, AVG(session_end - session_start) AS duration\n",
    "FROM twitch_sessions\n",
    "GROUP BY session_type"
   ],
   "id": "82224f2c5c12a630"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = twitch_sessions\n",
    "df['duration'] = df['session_end'] - df['session_start']\n",
    "df.groupby('session_type', as_index=False).agg(duration=('duration', 'mean'))"
   ],
   "id": "69f9f0eb93f89aba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2012\n",
    "\n",
    "```From users who had their first session as a viewer, how many streamer sessions have they had? Return the user id and number of sessions in descending order. In case there are users with the same number of sessions, order them by ascending user id.```"
   ],
   "id": "54ae160214dc8e73"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql12"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "WITH ranked AS (SELECT user_id,\n",
    "                       session_type,\n",
    "                       DENSE_RANK()\n",
    "                       OVER (PARTITION BY user_id ORDER BY session_start) AS rnk\n",
    "                FROM twitch_sessions),\n",
    "     first_viewer_users AS (SELECT user_id\n",
    "                            FROM ranked\n",
    "                            WHERE session_type = 'viewer'\n",
    "                              AND rnk = 1)\n",
    "SELECT user_id, COUNT(session_id) AS n_sessions\n",
    "FROM twitch_sessions\n",
    "WHERE user_id IN (SELECT user_id FROM first_viewer_users)\n",
    "  AND session_type = 'streamer'\n",
    "GROUP BY user_id\n",
    "ORDER BY n_sessions DESC, user_id ASC"
   ],
   "id": "2fc9a65bcc066b1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = twitch_sessions\n",
    "df['rnk'] = df.groupby('user_id')['session_start'].rank(method='first', ascending=True)\n",
    "first_viewer_users = df.query('session_type == \"viewer\" & rnk == 1')['user_id'].to_list()\n",
    "df.query('session_type == \"streamer\" & user_id.isin(@first_viewer_users)').groupby('user_id', as_index=False).agg(\n",
    "    n_sessions=('session_id', 'count')).sort_values(['n_sessions', 'user_id'], ascending=[False, True])"
   ],
   "id": "6402d71533ae91f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2013\n",
    "\n",
    "```How many customers placed an order and what is the average order amount?```"
   ],
   "id": "b31755f6f6e2a310"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql10"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT COUNT(DISTINCT customer_id) AS count,\n",
    "       AVG(amount)                 AS avg\n",
    "FROM postmates_orders"
   ],
   "id": "951fe1d1cd7861db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = postmates_orders\n",
    "df.agg({'customer_id': 'nunique', 'amount': 'mean'})"
   ],
   "id": "fe6eabcafd0d6ce6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2014\n",
    "\n",
    "```Which hour has the highest average order volume per day? Your output should have the hour which satisfies that condition, and average order volume.```"
   ],
   "id": "df69c5f52f1af527"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql11"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "WITH orders_count_by_day_hour AS (SELECT EXTRACT(HOUR FROM order_timestamp_utc) AS hour,\n",
    "                                         order_timestamp_utc::DATE              AS date,\n",
    "                                         COUNT(id)                              AS cnt\n",
    "                                  FROM postmates_orders\n",
    "                                  GROUP BY hour, date),\n",
    "     avg_orders_by_hour AS (SELECT hour,\n",
    "                                   AVG(cnt)                             AS avg_orders,\n",
    "                                   RANK() OVER (ORDER BY AVG(cnt) DESC) AS rnk\n",
    "                            FROM orders_count_by_day_hour\n",
    "                            GROUP BY hour)\n",
    "SELECT hour, avg_orders\n",
    "FROM avg_orders_by_hour\n",
    "WHERE rnk = 1"
   ],
   "id": "bd9d1777db8edd4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = postmates_orders\n",
    "df['hour'] = df['order_timestamp_utc'].dt.hour\n",
    "df['day'] = df['order_timestamp_utc'].dt.date\n",
    "df.groupby(['hour', 'day'], as_index=False).agg(cnt=('id', 'count')).groupby('hour', as_index=False).agg(\n",
    "    avg_orders=('cnt', 'mean')).nlargest(1, 'avg_orders', keep='all')"
   ],
   "id": "77e7b485da21b90b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2015\n",
    "\n",
    "```What cities recorded the largest growth and biggest drop in order amount between March 11, 2019, and April 11, 2019. Just compare order amounts on those two dates. Your output should include the names of the cities and the amount of growth/drop.```"
   ],
   "id": "7aa7fd44f21dbd2a"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql16"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "WITH total_amount_by_city_in_date AS (SELECT city_id,\n",
    "                                             order_timestamp_utc::DATE AS date,\n",
    "                                             SUM(amount) as total_amount\n",
    "                                      FROM postmates_orders\n",
    "                                      WHERE order_timestamp_utc BETWEEN '2019-03-11' AND '2019-04-12'\n",
    "                                      GROUP BY city_id, date),\n",
    "     shifted_total_amount AS (SELECT city_id,\n",
    "                                     date,\n",
    "                                     LAG(total_amount) OVER ()       AS prev_total_amount,\n",
    "                                     total_amount - LAG(total_amount) OVER () AS amount_diff\n",
    "                              FROM total_amount_by_city_in_date)\n",
    "SELECT name, amount_diff\n",
    "FROM shifted_total_amount AS ta\n",
    "         JOIN postmates_markets AS pm ON ta.city_id = pm.id\n",
    "WHERE amount_diff =\n",
    "      (SELECT MAX(amount_diff) FROM shifted_total_amount WHERE date = '2019-04-11')\n",
    "   OR amount_diff =\n",
    "      (SELECT MIN(amount_diff) FROM shifted_total_amount WHERE date = '2019-04-11')\n",
    "ORDER BY city_id, date"
   ],
   "id": "7d3718e9751f3cae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = postmates_orders\n",
    "df['date'] = df['order_timestamp_utc'].dt.date\n",
    "total_amount_by_city_in_date = df[(df['date'] == pd.to_datetime('2019-03-11').date()) |\n",
    "                                  (df['date'] == pd.to_datetime('2019-04-11').date())].groupby(['city_id', 'date'],\n",
    "                                                                                               as_index=False).agg(\n",
    "    total_amount=('amount', 'sum'))\n",
    "total_amount_by_city_in_date['prev_total_amount'] = total_amount_by_city_in_date['total_amount'].shift(1)\n",
    "total_amount_by_city_in_date['amount_diff'] = total_amount_by_city_in_date['total_amount'] - \\\n",
    "                                              total_amount_by_city_in_date['prev_total_amount']\n",
    "result = total_amount_by_city_in_date.drop_duplicates(subset='city_id', keep='last')\n",
    "max_amount_diff = result['amount_diff'].max()\n",
    "min_amount_diff = result['amount_diff'].min()\n",
    "result.query('amount_diff == @max_amount_diff | amount_diff == @min_amount_diff').merge(postmates_markets, how='inner',\n",
    "                                                                                        left_on='city_id',\n",
    "                                                                                        right_on='id')[\n",
    "    ['name', 'amount_diff']]"
   ],
   "id": "6ab92b441488a4b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2016\n",
    "\n",
    "```Which partners have ‘pizza’ in their name and are located in Boston? And what is the average order amount? Output the partner name and the average order amount.```"
   ],
   "id": "33a8b4d6ca3996a6"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql3"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT pp.name, AVG(po.amount) AS avg\n",
    "FROM postmates_orders po\n",
    "         LEFT JOIN postmates_markets pm ON po.city_id = pm.id\n",
    "         LEFT JOIN postmates_partners pp ON po.seller_id = pp.id\n",
    "WHERE pm.name = 'Boston'\n",
    "  AND pp.name ILIKE '%pizza%'\n",
    "GROUP BY pp.name"
   ],
   "id": "e7e03859d29f8276"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.merge(pd.merge(postmates_orders, postmates_markets, how='left', left_on='city_id', right_on='id'),\n",
    "              postmates_partners, how='left', left_on='seller_id', right_on='id')\n",
    "df.query('name_x == \"Boston\" & name_y.str.contains(\"pizza\", case=False)').groupby('name_y', as_index=False).agg(\n",
    "    avg=('amount', 'mean'))"
   ],
   "id": "308c7b8e335e48fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2018\n",
    "\n",
    "```Return a list of users with status free who didn’t make any calls in Apr 2020.```"
   ],
   "id": "950b5051a6adbe26"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql14"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT user_id\n",
    "FROM rc_users\n",
    "WHERE user_id NOT IN (SELECT user_id\n",
    "                      FROM rc_calls\n",
    "                      WHERE EXTRACT(MONTH FROM date) = 4\n",
    "                        AND EXTRACT(YEAR FROM date) = 2020)\n",
    "  AND status = 'free'"
   ],
   "id": "3e04c2d6cc5c3dbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "user_lst = rc_calls.query('date.dt.year == 2020 & date.dt.month == 4')['user_id'].to_list()\n",
    "rc_users.query('~user_id.isin(@user_lst) & status == \"free\"')['user_id']"
   ],
   "id": "4acaa2445b9ce739"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2019\n",
    "\n",
    "```Return the top 2 users in each company that called the most. Output the company_id, user_id, and the user's rank. If there are multiple users in the same rank, keep all of them.```"
   ],
   "id": "520e9ded366b1123"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql17"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "WITH cte AS (SELECT company_id,\n",
    "                    user_id,\n",
    "                    COUNT(call_id)                                              AS cnt,\n",
    "                    DENSE_RANK()\n",
    "                    OVER (PARTITION BY company_id ORDER BY COUNT(call_id) DESC) AS rank\n",
    "             FROM rc_calls\n",
    "                      JOIN rc_users USING (user_id)\n",
    "             GROUP BY company_id, user_id)\n",
    "SELECT company_id,\n",
    "       user_id,\n",
    "       rank\n",
    "FROM cte\n",
    "WHERE rank <= 2;"
   ],
   "id": "f75d73d52c7b8ab5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.merge(rc_calls, rc_users, how='inner', on='user_id').groupby(['company_id', 'user_id'], as_index=False).agg(\n",
    "    cnt=('call_id', 'count'))\n",
    "df['rnk'] = df.groupby('company_id')['cnt'].rank(method='dense', ascending=False)\n",
    "df.query('rnk <= 2')[['company_id', 'user_id', 'rnk']]"
   ],
   "id": "a92b09ae68b37b26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ID 2020\n",
    "\n",
    "```Which company had the biggest month call decline from March to April 2020? Return the company_id and calls difference for the company with the highest decline.```"
   ],
   "id": "db6780428f135970"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
